{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple gradient-based quadratic classification model to compute $W$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install torch scikit-learn matplotlib -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from src.models.quadratic import QuadraticModel, QuadMLP\n",
    "from src.models.trainer import ModelTrainer\n",
    "from src.config.config import cfg\n",
    "\n",
    "np.set_printoptions(precision=2, suppress=True, linewidth=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp = {\n",
    "    \"data_path\": \"../data/synth/encoded_founders_composites.csv\",\n",
    "    \"target_column\": \"success\",\n",
    "    \"test_size\": 0.2,\n",
    "    \"val_size\": 0.25,\n",
    "    \"random_state\": 4,\n",
    "    \"batch_size\": 64,\n",
    "    \"lr\": 0.001,\n",
    "    \"weight_decay\": 1e-2,\n",
    "    \"epochs\": 200,\n",
    "    \"patience\": 10,\n",
    "    \"lr_decay_factor\": 0.5,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(hyp[\"data_path\"])\n",
    "\n",
    "feature_columns = df.columns[:-3]\n",
    "\n",
    "X = df[feature_columns].to_numpy()\n",
    "X = StandardScaler().fit_transform(X)\n",
    "y = df[hyp[\"target_column\"]].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=hyp[\"test_size\"], random_state=hyp[\"random_state\"], shuffle=True)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=hyp[\"val_size\"], random_state=hyp[\"random_state\"], shuffle=True)\n",
    "\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_tensor = torch.FloatTensor(y_train)\n",
    "X_val_tensor = torch.FloatTensor(X_val)\n",
    "y_val_tensor = torch.FloatTensor(y_val)\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_test_tensor = torch.FloatTensor(y_test)\n",
    "\n",
    "pos_weight = torch.tensor([(y_train == 0).sum() / (y_train == 1).sum()])\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=hyp[\"batch_size\"])\n",
    "\n",
    "train_loss_history, val_loss_history, test_loss_history = [], [], []\n",
    "train_acc_history, val_acc_history, test_acc_history = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "model = QuadMLP(input_dim, hidden_dim=32, rand_init=False)\n",
    "# model = QuadraticModel(input_dim=input_dim)\n",
    "W_init = model.W.clone().detach()\n",
    "\n",
    "trainer = ModelTrainer(model, hyp, pos_weight)\n",
    "\n",
    "# Train the model\n",
    "trainer.train(train_loader, X_val_tensor, y_val_tensor, X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test_tensor.to(hyp[\"device\"]))\n",
    "    test_preds = (test_outputs > 0).int().cpu().numpy()\n",
    "\n",
    "W_final = model.get_W().detach().cpu().numpy()\n",
    "b_final = model.b.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(trainer.train_loss_history, label=\"Train Loss\")\n",
    "plt.plot(trainer.val_loss_history, label=\"Validation Loss\")\n",
    "plt.plot(trainer.test_loss_history, label=\"Test Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Cross-Entropy Loss\")\n",
    "plt.title(\"Loss Over Epochs\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Accuracy curves\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(trainer.train_acc_history, label=\"Train Accuracy\")\n",
    "plt.plot(trainer.val_acc_history, label=\"Validation Accuracy\")\n",
    "plt.plot(trainer.test_acc_history, label=\"Test Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy Over Epochs\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "report = classification_report(y_test, test_preds, output_dict=True)\n",
    "report_df = pd.DataFrame(report).round(3).T\n",
    "report_df = report_df.drop(\"support\", axis=1)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.heatmap(report_df, annot=True, cmap=\"YlOrRd\", fmt=\".3f\", cbar=False)\n",
    "plt.title(\"Classification Report Metrics\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmin, vmax = -16, 16\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
    "im0 = axs[0].imshow(W_init.cpu().numpy(), aspect=\"auto\", cmap=\"viridis\", vmin=vmin, vmax=vmax)\n",
    "axs[0].set_title(\"Initial Weight Matrix\")\n",
    "plt.colorbar(im0, ax=axs[0])\n",
    "\n",
    "im1 = axs[1].imshow(W_final, aspect=\"auto\", cmap=\"viridis\", vmin=vmin, vmax=vmax)\n",
    "axs[1].set_title(\"Final Weight Matrix\")\n",
    "plt.colorbar(im1, ax=axs[1])\n",
    "\n",
    "W_diff = W_final - W_init.cpu().numpy()\n",
    "im2 = axs[2].imshow(W_diff, aspect=\"auto\", cmap=\"RdBu\")\n",
    "axs[2].set_title(\"Change in Weight Matrix\")\n",
    "plt.colorbar(im2, ax=axs[2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "\n",
    "filename = \"../models/26x26/W_1.pkl\"\n",
    "new_filename = re.sub(r\"W_(\\d+)\", lambda x: f\"W_{int(x.group(1)) + 1}\", filename)\n",
    "\n",
    "with open(new_filename, \"wb\") as f:\n",
    "    pickle.dump(W_final, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Feature Importance\n",
    "feature_importance = np.abs(np.diag(W_final))\n",
    "feature_pairs = np.abs(W_final - np.diag(np.diag(W_final)))\n",
    "feature_names = df[feature_columns].columns\n",
    "\n",
    "feature_importance_abs = np.abs(np.diag(W_final))\n",
    "feature_importance_real = np.diag(W_final)\n",
    "feature_importance_normalized = feature_importance_real / np.max(np.abs(feature_importance_real))\n",
    "\n",
    "importance_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Feature\": feature_names,\n",
    "        \"Importance\": feature_importance_normalized,\n",
    "        \"Importance_abs\": np.abs(feature_importance_normalized),\n",
    "    }\n",
    ").sort_values(\"Importance_abs\", ascending=True)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=importance_df, y=\"Feature\", x=\"Importance\")\n",
    "plt.title(\"Normalized Feature Importance (Diagonal Values)\")\n",
    "plt.axvline(x=0, color=\"black\", linestyle=\"-\", alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MATRIX = cfg.MATRIX\n",
    "category_map = {}\n",
    "start_idx = 0\n",
    "for cat in MATRIX:\n",
    "    dim = MATRIX[cat][\"DIMENSION\"]\n",
    "    end_idx = start_idx + dim\n",
    "    for i in range(start_idx, end_idx):\n",
    "        category_map[feature_names[i]] = cat\n",
    "    start_idx = end_idx\n",
    "\n",
    "upper_triangle = np.triu(W_final, k=1)\n",
    "\n",
    "pairs = []\n",
    "for i in range(len(upper_triangle)):\n",
    "    for j in range(i + 1, len(upper_triangle)):\n",
    "        cat1 = category_map[feature_names[i]]\n",
    "        cat2 = category_map[feature_names[j]]\n",
    "        if cat1 != cat2:  # Only include pairs from different categories\n",
    "            pairs.append((feature_names[i], feature_names[j], upper_triangle[i, j]))\n",
    "\n",
    "\n",
    "interactions_df = pd.DataFrame(pairs, columns=[\"Feature 1\", \"Feature 2\", \"Interaction Strength\"])\n",
    "max_abs_interaction = np.max(np.abs(interactions_df[\"Interaction Strength\"]))\n",
    "interactions_df[\"Interaction Strength\"] = interactions_df[\"Interaction Strength\"] / max_abs_interaction\n",
    "interactions_df[\"Abs_Strength\"] = np.abs(interactions_df[\"Interaction Strength\"])\n",
    "interactions_df = interactions_df.sort_values(\"Abs_Strength\", ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_n = 15\n",
    "top_interactions = interactions_df.head(top_n)\n",
    "\n",
    "colors = [\"red\" if x < 0 else \"blue\" for x in top_interactions[\"Interaction Strength\"]]\n",
    "sns.barplot(\n",
    "    data=top_interactions,\n",
    "    y=top_interactions.apply(lambda x: f\"{x['Feature 1']} Ã— {x['Feature 2']}\", axis=1),\n",
    "    x=\"Interaction Strength\",\n",
    "    palette=colors,\n",
    ")\n",
    "\n",
    "plt.title(f\"Top {top_n} Normalized Cross-Category Feature Interactions\")\n",
    "plt.xlabel(\"Normalized Interaction Strength\")\n",
    "plt.ylabel(\"Feature Pair\")\n",
    "plt.axvline(x=0, color=\"black\", linestyle=\"-\", alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "mask = np.zeros_like(W_final)\n",
    "mask[np.tril_indices_from(mask, k=-1)] = True\n",
    "\n",
    "sns.heatmap(\n",
    "    W_final,\n",
    "    xticklabels=feature_names,\n",
    "    yticklabels=feature_names,\n",
    "    cmap=\"RdBu\",\n",
    "    center=0,\n",
    "    mask=mask,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    square=True,\n",
    ")\n",
    "\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.yticks(rotation=0)\n",
    "plt.title(\"Feature Interaction Matrix (Upper Triangle)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model predictions and confidence\n",
    "with torch.no_grad():\n",
    "    X_test_tensor = X_test_tensor.to(hyp[\"device\"])\n",
    "    logits = model(X_test_tensor)\n",
    "    probs = torch.sigmoid(logits).cpu().numpy()\n",
    "    preds = (probs > 0.5).astype(int)\n",
    "\n",
    "analysis_df = pd.DataFrame(X_test, columns=feature_names)\n",
    "analysis_df[\"true_label\"] = y_test\n",
    "analysis_df[\"predicted\"] = preds\n",
    "analysis_df[\"confidence\"] = np.abs(probs - 0.5) + 0.5\n",
    "analysis_df[\"correct\"] = analysis_df[\"true_label\"] == analysis_df[\"predicted\"]\n",
    "\n",
    "n_features = len(feature_names)\n",
    "n_rows = (n_features + 3) // 4\n",
    "\n",
    "plt.figure(figsize=(20, 5 * n_rows))\n",
    "\n",
    "for i, feature in enumerate(feature_names):\n",
    "    plt.subplot(n_rows, 4, i + 1)\n",
    "    sns.kdeplot(data=analysis_df[analysis_df[\"correct\"]], x=feature, label=\"Correct\", alpha=0.6)\n",
    "    sns.kdeplot(data=analysis_df[~analysis_df[\"correct\"]], x=feature, label=\"Incorrect\", alpha=0.6)\n",
    "    plt.title(f\"{feature}\")\n",
    "    plt.xlabel(\"Standardized Value\")\n",
    "    if i == 0:\n",
    "        plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and show means for all features\n",
    "confidence_threshold = np.percentile(analysis_df[\"confidence\"], 75)\n",
    "high_conf_correct = analysis_df[(analysis_df[\"correct\"]) & (analysis_df[\"confidence\"] >= confidence_threshold)]\n",
    "high_conf_incorrect = analysis_df[(~analysis_df[\"correct\"]) & (analysis_df[\"confidence\"] >= confidence_threshold)]\n",
    "\n",
    "comparison_df = pd.DataFrame(\n",
    "    {\n",
    "        \"High Conf Correct\": high_conf_correct[feature_names].mean(),\n",
    "        \"High Conf Incorrect\": high_conf_incorrect[feature_names].mean(),\n",
    "        \"All Data\": analysis_df[feature_names].mean(),\n",
    "    }\n",
    ").round(3)\n",
    "\n",
    "print(\"\\nFeature Means for High Confidence Predictions:\")\n",
    "print(comparison_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
