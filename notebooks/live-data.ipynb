{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install bs4 selenium webdriver-manager -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.clients.perplexity_client import PerplexityClient\n",
    "from src.clients.proxycurl_client import ProxycurlClient\n",
    "from src.config.config import cfg\n",
    "from src.processing.transforms import ProfileTransforms\n",
    "from src.clients.yc_client import YCClient\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping (www.ycombinator.com/companies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "yc = YCClient()\n",
    "\n",
    "batch_codes = []\n",
    "\n",
    "# range [i, j] is 200i to 200j.  W and S.\n",
    "for year in range(21, 17, -1):\n",
    "    yr = str(year).zfill(2)\n",
    "    batch_codes.extend([f\"W{yr}\", f\"S{yr}\"])\n",
    "\n",
    "\n",
    "output_dir = \"../data/live/yc\"\n",
    "\n",
    "# df = yc.scrape_batches(batch_codes, output_dir=None)\n",
    "\n",
    "yc.__del__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get exits and funding for companies\n",
    "\n",
    "Some of these numbers, especially exits are off. But this data should get close enough to map to ordinals and success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = PerplexityClient()\n",
    "\n",
    "batch_codes = ['W17', 'S17']\n",
    "for bc in batch_codes:\n",
    "    try:\n",
    "        filename = f'{output_dir}/{bc}.csv'\n",
    "        \n",
    "        batch_df = pd.read_csv(filename)\n",
    "        batch_df = batch_df.drop_duplicates(subset=['Name', 'LinkedIn'])\n",
    "        batch_df = batch_df.dropna(subset=['Name', 'LinkedIn'])\n",
    "        \n",
    "        for company in batch_df['Company'].unique():\n",
    "            try:\n",
    "                evaluation = pc.eval_company(f'{company} (YC {bc})')\n",
    "                \n",
    "                batch_df.loc[batch_df['Company'] == company, 'exit_value_usd'] = evaluation.get('exit_value_usd')\n",
    "                batch_df.loc[batch_df['Company'] == company, 'total_funding_usd'] = evaluation.get('total_funding_usd')\n",
    "                \n",
    "                batch_df.to_csv(filename, index=False)\n",
    "                print(f\"Processed {company}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error evaluating {company} (YC {bc}): {e}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {bc}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MATRIX = cfg.MATRIX\n",
    "BATCH_SIZE = 5  \n",
    "LINKEDIN_PROFILES_PATH = \"../data/linkedin_profiles.json\"\n",
    "ENCODED_DATA_DIR = \"../data/raw/\"\n",
    "YC_DATA_DIR = \"../data/live/yc\"\n",
    "CUTOFF_YEAR = 2017  \n",
    "\n",
    "def load_existing_profiles():\n",
    "    try:\n",
    "        with open(LINKEDIN_PROFILES_PATH, 'r') as f:\n",
    "            profiles = json.load(f)\n",
    "        print(f\"Loaded {len(profiles)} existing profiles\")\n",
    "        return profiles\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        print(\"Starting with empty profiles list\")\n",
    "        return []\n",
    "\n",
    "def get_processed_urls(profiles):\n",
    "    urls = set()\n",
    "    for p in profiles:\n",
    "        # Check different possible URL fields\n",
    "        for field in ['input_linkedin_url', 'linkedin_url', 'public_url', 'url']:\n",
    "            if field in p and p[field]:\n",
    "                urls.add(p[field])\n",
    "                break\n",
    "    return urls\n",
    "\n",
    "def fetch_linkedin_profile(url, api_key):\n",
    "    headers = {'Authorization': 'Bearer ' + api_key}\n",
    "    api_endpoint = 'https://nubela.co/proxycurl/api/v2/linkedin'\n",
    "    params = {\n",
    "        'linkedin_profile_url': url,\n",
    "        'use_cache': 'if-present',\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(api_endpoint, params=params, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200 and response.content:\n",
    "            profile = response.json()\n",
    "            profile['input_linkedin_url'] = url\n",
    "            return profile\n",
    "        else:\n",
    "            print(f\"Error fetching profile {url}: Status {response.status_code}\")\n",
    "            return {\n",
    "                'input_linkedin_url': url,\n",
    "                'public_identifier': url.split('/')[-1].split('?')[0],\n",
    "                'error': f'API Error: {response.status_code}'\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"Exception fetching profile {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def save_profiles(profiles):\n",
    "    with open(LINKEDIN_PROFILES_PATH, 'w') as f:\n",
    "        json.dump(profiles, f)\n",
    "\n",
    "def get_yc_batch_files():\n",
    "    return [f for f in os.listdir(YC_DATA_DIR) if f.endswith('.csv')]\n",
    "\n",
    "def process_batch_file(file_path, linkedin_profiles, processed_urls):\n",
    "    print(f\"Processing {file_path}...\")\n",
    "    \n",
    "    # Load founders data\n",
    "    founders = pd.read_csv(file_path)\n",
    "    founders = founders.dropna(subset=['LinkedIn'])\n",
    "    \n",
    "    if 'exit_value_usd' in founders.columns and 'total_funding_usd' in founders.columns:\n",
    "        founders = founders[~((founders['exit_value_usd'] == 0) & (founders['total_funding_usd'] == 0))]\n",
    "    \n",
    "    batch_code = os.path.basename(file_path).split('.')[0]\n",
    "    \n",
    "    new_profiles = []\n",
    "    \n",
    "    for idx, row in founders.iterrows():\n",
    "        linkedin_url = row['LinkedIn']\n",
    "        \n",
    "        if linkedin_url in processed_urls:\n",
    "            print(f\"Skipping already processed profile: {linkedin_url}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Fetching profile {idx+1}/{len(founders)}: {linkedin_url}\")\n",
    "        \n",
    "        api_key = os.getenv('PROXYCURL_API_KEY')\n",
    "        profile = fetch_linkedin_profile(linkedin_url, api_key)\n",
    "        \n",
    "        if profile:\n",
    "            profile['yc_batch'] = batch_code\n",
    "            \n",
    "            profile['company_name'] = row.get('Company', '')\n",
    "            if 'exit_value_usd' in row:\n",
    "                profile['exit_value_usd'] = row['exit_value_usd']\n",
    "            if 'total_funding_usd' in row:\n",
    "                profile['total_funding_usd'] = row['total_funding_usd']\n",
    "            \n",
    "            linkedin_profiles.append(profile)\n",
    "            new_profiles.append(profile)\n",
    "            processed_urls.add(linkedin_url)\n",
    "            \n",
    "            if len(new_profiles) % BATCH_SIZE == 0:\n",
    "                print(f\"Saving batch with {len(new_profiles)} new profiles (total: {len(linkedin_profiles)})...\")\n",
    "                save_profiles(linkedin_profiles)\n",
    "        \n",
    "        time.sleep(1)\n",
    "    \n",
    "    if new_profiles:\n",
    "        save_profiles(linkedin_profiles)\n",
    "        print(f\"Saved {len(linkedin_profiles)} profiles to {LINKEDIN_PROFILES_PATH} ({len(new_profiles)} new from {batch_code})\")\n",
    "    \n",
    "    return linkedin_profiles, processed_urls\n",
    "\n",
    "def transform_and_encode_profiles(linkedin_profiles, batch_code=None):\n",
    "    T = ProfileTransforms(data={}, matrix=MATRIX)\n",
    "    \n",
    "    if batch_code:\n",
    "        batch_profiles = [p for p in linkedin_profiles if p.get('yc_batch') == batch_code]\n",
    "        print(f\"Processing {len(batch_profiles)} profiles for batch {batch_code}\")\n",
    "    else:\n",
    "        batch_profiles = linkedin_profiles\n",
    "        print(f\"Processing all {len(batch_profiles)} profiles\")\n",
    "    \n",
    "    valid_profiles = []\n",
    "    for profile in batch_profiles:\n",
    "        if profile and 'experiences' in profile and profile['experiences'] is not None:\n",
    "            valid_profiles.append(profile)\n",
    "        else:\n",
    "            print(f\"Skipping invalid profile: {profile.get('input_linkedin_url', 'unknown URL')}\")\n",
    "    \n",
    "    print(f\"Found {len(valid_profiles)} valid profiles out of {len(batch_profiles)}\")\n",
    "    \n",
    "    df = T.transform_person_endpt(profile_list=valid_profiles, cutoff_date=CUTOFF_YEAR)\n",
    "    T.df = df\n",
    "    \n",
    "    pc = PerplexityClient()\n",
    "    \n",
    "    print(\"Starting AI evaluations with Perplexity API...\")\n",
    "    total_profiles = len(df)\n",
    "    \n",
    "    ai_evaluations = []\n",
    "    for idx, row in df.iterrows():\n",
    "        print(f\"Evaluating profile {idx+1}/{total_profiles}: {row.get('Name', 'Unknown')}\")\n",
    "        try:\n",
    "            evaluation = pc.eval_person(row, MATRIX)\n",
    "            ai_evaluations.append(evaluation)\n",
    "            time.sleep(0.5)\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating profile {idx+1}: {e}\")\n",
    "            ai_evaluations.append({\"exited_founder\": 0, \"previous_founder\": 1, \"startup_experience\": 1})\n",
    "    \n",
    "    df[\"EXIT\"] = [x.get(\"exited_founder\", 0) for x in ai_evaluations]\n",
    "    df[\"FOUNDER\"] = [x.get(\"previous_founder\", 1) for x in ai_evaluations]\n",
    "    df[\"STARTUP\"] = [x.get(\"startup_experience\", 1) for x in ai_evaluations]\n",
    "    \n",
    "    print(\"AI evaluations completed.\")\n",
    "    \n",
    "    T._add_ordinal_columns()\n",
    "\n",
    "    print(\"Creating feature matrix...\")\n",
    "    feature_matrix = T.create_feature_matrix()\n",
    "    df[\"feature_vector\"] = list(feature_matrix)\n",
    "    \n",
    "    output_path = os.path.join(ENCODED_DATA_DIR, f\"{batch_code}_profiles.csv\")\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"Saved encoded profiles to {output_path}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "linkedin_profiles = load_existing_profiles()\n",
    "processed_urls = get_processed_urls(linkedin_profiles)\n",
    "print(f\"Found {len(linkedin_profiles)} existing profiles with {len(processed_urls)} unique URLs\")\n",
    "\n",
    "if linkedin_profiles:\n",
    "    print(\"\\nDebug: First profile structure\")\n",
    "    first_profile = linkedin_profiles[0]\n",
    "    print(f\"Keys: {list(first_profile.keys())[:10]}...\")\n",
    "    \n",
    "    url_fields = ['input_linkedin_url', 'linkedin_url', 'public_url', 'url']\n",
    "    for field in url_fields:\n",
    "        if field in first_profile:\n",
    "            print(f\"Found URL field: {field} = {first_profile[field]}\")\n",
    "batch_files = get_yc_batch_files()\n",
    "print(f\"\\nFound {len(batch_files)} YC batch files: {batch_files}\")\n",
    "\n",
    "\n",
    "target_batch = \"W17.csv\"\n",
    "if target_batch in batch_files:\n",
    "    file_path = os.path.join(YC_DATA_DIR, target_batch)\n",
    "    linkedin_profiles, processed_urls = process_batch_file(file_path, linkedin_profiles, processed_urls)\n",
    "    \n",
    "    batch_code = target_batch.split('.')[0]\n",
    "    \n",
    "    encoded_df = transform_and_encode_profiles(linkedin_profiles, batch_code)\n",
    "    print(f\"Encoded {len(encoded_df)} profiles for batch {batch_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from src.clients.perplexity_client import PerplexityClient\n",
    "\n",
    "with open('../data/linkedin_profiles.json', 'r') as f:\n",
    "    linkedin_profiles = json.load(f)\n",
    "print(f\"Loaded {len(linkedin_profiles)} LinkedIn profiles\")\n",
    "\n",
    "top_companies = pd.read_csv('../data/live/yc/top_companies.csv')\n",
    "print(f\"Processing {len(top_companies)} top companies\")\n",
    "\n",
    "pc = PerplexityClient()\n",
    "processed_profiles = []\n",
    "\n",
    "for idx, company in top_companies.iterrows():\n",
    "    print(f\"\\nProcessing {idx+1}/{len(top_companies)}: {company['Company']}\")\n",
    "    \n",
    "    profile = None\n",
    "    linkedin_url = company['LinkedIn'].lower().strip().rstrip('/')\n",
    "    \n",
    "    for p in linkedin_profiles:\n",
    "        if any(url and linkedin_url in str(url).lower().strip().rstrip('/') \n",
    "               for url in [p.get('linkedin_profile_url'), p.get('public_identifier'), \n",
    "                         p.get('input_linkedin_url'), p.get('public_url')]):\n",
    "            profile = p\n",
    "            break\n",
    "    \n",
    "    if not profile:\n",
    "        print(f\"No profile found for {linkedin_url}\")\n",
    "        continue\n",
    "\n",
    "    transforms = ProfileTransforms({}, cfg.MATRIX)\n",
    "    cutoff_date = company['cutoff_date'] if pd.notna(company['cutoff_date']) else None\n",
    "    \n",
    "    try:\n",
    "        processed = transforms.process_person_endpt(profile, cutoff_date)\n",
    "        if not processed:\n",
    "            print(\"Failed to process profile\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            evaluation = pc.eval_person(processed, cfg.MATRIX)\n",
    "            time.sleep(0.5)  \n",
    "        except Exception as e:\n",
    "            print(f\"Error in AI evaluation: {e}\")\n",
    "            evaluation = {\"exited_founder\": 0, \"previous_founder\": 1, \"startup_experience\": 1}\n",
    "        \n",
    "\n",
    "        processed.update({\n",
    "            'Company': company['Company'],\n",
    "            'exit_value_usd': company['exit_value_usd'],\n",
    "            'total_funding_usd': company['total_funding_usd'],\n",
    "            'EXIT': max(2 if company['exit_value_usd'] > 0 else 0, evaluation.get('exited_founder', 0)),\n",
    "            'FOUNDER': max(3, evaluation.get('previous_founder', 1)),  # Top company founder\n",
    "            'STARTUP': max(3, evaluation.get('startup_experience', 1))  # Successful startup\n",
    "        })\n",
    "        \n",
    "\n",
    "        transforms.df = pd.DataFrame([processed])\n",
    "        transforms._add_ordinal_columns()\n",
    "        feature_matrix = transforms.create_feature_matrix()\n",
    "        processed['feature_vector'] = feature_matrix[0].tolist()\n",
    "        \n",
    "        processed_profiles.append(processed)\n",
    "        print(f\"Successfully processed {processed['Name']}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing profile: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "results_df = pd.DataFrame(processed_profiles)\n",
    "\n",
    "output_path = '../data/raw/top_companies_profiles.csv'\n",
    "results_df.to_csv(output_path, index=False)\n",
    "print(f\"\\nProcessed {len(results_df)} profiles successfully\")\n",
    "print(f\"Saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def process_batch_data(batch_code):\n",
    "    save_df = pd.read_csv('../data/synth/encoded_founders_composites.csv')\n",
    "    \n",
    "    profiles_path = f'../data/raw/{batch_code}_profiles.csv'\n",
    "    profiles_df = pd.read_csv(profiles_path)\n",
    "    \n",
    "    funding_path = f'../data/live/yc/{batch_code}.csv'\n",
    "    funding_df = pd.read_csv(funding_path)\n",
    "    \n",
    "    result_df = pd.DataFrame(columns=save_df.columns)\n",
    "    \n",
    "    SUCCESS_FUNDING_THRESHOLD = cfg.SUCCESS_FUNDING_THRESHOLD\n",
    "    \n",
    "    funding_df['normalized_name'] = funding_df['Name'].apply(normalize_name)\n",
    "    \n",
    "    normalized_funding_names = list(funding_df['normalized_name'])\n",
    "    name_to_idx = {name: idx for idx, name in enumerate(normalized_funding_names) if name}\n",
    "    \n",
    "    match_log = []\n",
    "    \n",
    "    for index, row in profiles_df.iterrows():\n",
    "        feature_str = row['feature_vector']\n",
    "        name = row['Name']\n",
    "        normalized_name = normalize_name(name)\n",
    "        \n",
    "        feature_str = feature_str.replace('[', '').replace(']', '').replace(',', ' ')\n",
    "        feature_values = [float(x) for x in feature_str.split() if x.strip()]\n",
    "        \n",
    "        new_row = {}\n",
    "        \n",
    "        for i, col in enumerate(save_df.columns[:26]):\n",
    "            if i < len(feature_values):\n",
    "                new_row[col] = feature_values[i]\n",
    "        \n",
    "        funding_data, match_type, matched_name = find_matching_funding_data(\n",
    "            normalized_name, funding_df, normalized_funding_names, name_to_idx\n",
    "        )\n",
    "        \n",
    "        match_log.append({\n",
    "            'profile_name': name,\n",
    "            'matched_funding_name': matched_name,\n",
    "            'match_type': match_type\n",
    "        })\n",
    "        \n",
    "        new_row['exit_value'] = funding_data.iloc[0]['exit_value_usd']\n",
    "        new_row['funding_amount'] = funding_data.iloc[0]['total_funding_usd']\n",
    "        \n",
    "        exit_value = funding_data.iloc[0]['exit_value_usd']\n",
    "        funding_amount = funding_data.iloc[0]['total_funding_usd']\n",
    "        new_row['success'] = 1 if (exit_value > 0 or funding_amount > SUCCESS_FUNDING_THRESHOLD) else 0\n",
    "        \n",
    "        result_df = pd.concat([result_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "    \n",
    "    print_summary_statistics(result_df, match_log)\n",
    "    \n",
    "    output_path = f'../data/encoded/{batch_code}_encoded_with_outcomes.csv'\n",
    "    result_df.to_csv(output_path, index=False)\n",
    "    print(f\"Saved results to {output_path}\")\n",
    "    \n",
    "    match_log_df = pd.DataFrame(match_log)\n",
    "    \n",
    "    return result_df, match_log_df\n",
    "def normalize_name(name):\n",
    "    if not isinstance(name, str):\n",
    "        return \"\"\n",
    "    name = re.sub(r'[^\\w\\s]', '', name)\n",
    "    return name.lower().strip()\n",
    "\n",
    "def find_matching_funding_data(normalized_name, funding_df, normalized_funding_names, name_to_idx):\n",
    "   \n",
    "    from difflib import get_close_matches\n",
    "    \n",
    "    funding_data = funding_df[funding_df['normalized_name'] == normalized_name]\n",
    "    \n",
    "    if not funding_data.empty:\n",
    "        match_type = \"exact\"\n",
    "        matched_name = funding_data.iloc[0]['Name']\n",
    "    else:\n",
    "        if normalized_name:\n",
    "            close_matches = get_close_matches(normalized_name, normalized_funding_names, n=3, cutoff=0.6)\n",
    "            \n",
    "            if close_matches:\n",
    "                best_match = close_matches[0]\n",
    "                match_idx = name_to_idx[best_match]\n",
    "                funding_data = funding_df.iloc[[match_idx]]\n",
    "                match_type = \"fuzzy\"\n",
    "                matched_name = funding_data.iloc[0]['Name']\n",
    "            else:\n",
    "                first_word = normalized_name.split()[0] if normalized_name.split() else \"\"\n",
    "                if first_word:\n",
    "                    first_word_matches = [name for name in normalized_funding_names if name.startswith(first_word)]\n",
    "                    if first_word_matches:\n",
    "                        best_match = first_word_matches[0]\n",
    "                        match_idx = name_to_idx[best_match]\n",
    "                        funding_data = funding_df.iloc[[match_idx]]\n",
    "                        match_type = \"first_word\"\n",
    "                        matched_name = funding_data.iloc[0]['Name']\n",
    "                    else:\n",
    "                        funding_data = funding_df.iloc[[0]]\n",
    "                        match_type = \"default\"\n",
    "                        matched_name = funding_data.iloc[0]['Name']\n",
    "                else:\n",
    "                    funding_data = funding_df.iloc[[0]]\n",
    "                    match_type = \"default\"\n",
    "                    matched_name = funding_data.iloc[0]['Name']\n",
    "        else:\n",
    "            funding_data = funding_df.iloc[[0]]\n",
    "            match_type = \"default\"\n",
    "            matched_name = funding_data.iloc[0]['Name']\n",
    "    \n",
    "    return funding_data, match_type, matched_name\n",
    "\n",
    "def print_summary_statistics(result_df, match_log):\n",
    "\n",
    "    print(f\"Processed {len(result_df)} profiles\")\n",
    "    print(f\"Found funding data for {result_df['funding_amount'].notna().sum()} profiles\")\n",
    "    print(f\"Found exit data for {result_df['exit_value'].notna().sum()} profiles\")\n",
    "    print(f\"Successful companies: {result_df['success'].sum()}\")\n",
    "\n",
    "    print(\"\\nMatch Verification (first 20 entries):\")\n",
    "    for i, match in enumerate(match_log[:20]):\n",
    "        print(f\"{i+1}. {match['profile_name']} → {match['matched_funding_name']} ({match['match_type']})\")\n",
    "\n",
    "\n",
    "    match_types = pd.Series([m['match_type'] for m in match_log]).value_counts()\n",
    "    print(\"\\nMatch type distribution:\")\n",
    "    for match_type, count in match_types.items():\n",
    "        print(f\"{match_type}: {count} ({count/len(match_log)*100:.1f}%)\")\n",
    "\n",
    "\n",
    "batch_code = \"top_companies\" \n",
    "result_df, match_log_df = process_batch_data(batch_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
